{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asur/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-18 22:06:21,296] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/mnt/ssd/models/rugpt3small_based_on_gpt2\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strarting to process folder 'data/Freud':\n",
      "Process file 'Freyd Zigmund. Analiz konechnyy i beskonechnyy.txt'...\n",
      "Process file 'Freyd Zigmund. O narcizme.txt'...\n",
      "Process file 'Freyd Zigmund. Stroki biografii.txt'...\n",
      "Process file 'Freyd Zigmund. Znamenitye sluchai iz praktiki.txt'...\n",
      "Process file 'Freyd Zigmund. O psihoanalize.txt'...\n",
      "Process file 'Freyd Zigmund. Etot chelovek Moisey.txt'...\n",
      "Process file 'Freyd Zigmund. Ocherk istorii psihoanaziza.txt'...\n",
      "Process file 'Freyd Zigmund. Metapsihologicheskoe dopolnenie k ucheniyu o snovideniyah.txt'...\n",
      "Process file 'Freyd Zigmund. Nedovolstvo kulturoy.txt'...\n",
      "Process file 'Freyd Zigmund. Vospominanie vosproizvedenie i pererabotka.txt'...\n",
      "Process file 'Freyd Zigmund. Ocherki po psihologii seksualnosti.txt'...\n",
      "Process file 'Freyd Zigmund. Vvedenie V Psihoanaliz. Lekcii.txt'...\n",
      "Process file 'Freyd Zigmund. Rebenka byut - k voprosu o proishozhdenii seksualnyh izvrascheniy.txt'...\n",
      "Process file 'Freyd Zigmund. Psihopatologiya obydennoy zhizni.txt'...\n",
      "Process file 'Freyd Zigmund. Pechal i melanholiya.txt'...\n",
      "Process file 'Freyd Zigmund. Analiz fobii pyatiletnego malchika.txt'...\n",
      "Process file 'Freyd Zigmund. Vlecheniya i ih sudba.txt'...\n",
      "Process file 'Freyd Zigmund. Ya i Ono.txt'...\n",
      "Process file 'Freyd Zigmund. Neskolko zamechaniy po povodu ponyatiya bessoznatelnoe.txt'...\n",
      "Process file 'Freyd Zigmund. Trudnost na puti psihoanaliza.txt'...\n",
      "Process file 'Freyd Zigmund. Zigmund Freyd. Massovaya psihologiya i analiz chelovecheskogo Ya.txt'...\n",
      "Process file 'Freyd Zigmund. Totem i tabu. Psihologiya pervobytnoy kultury i religii.txt'...\n",
      "Process file 'Freyd Zigmund. Harakter i analnaya erotika.txt'...\n",
      "Process file 'Freyd Zigmund. Bessoznatelnoe.txt'...\n",
      "Process file 'Freyd Zigmund. Konechnyy i beskonechnyy analiz.txt'...\n",
      "Process file 'Freyd Zigmund. Nekotorye psihicheskie sledstviya anatomicheskogo razlichiya polov.txt'...\n",
      "Process file 'Freyd Zigmund. Polozhenie o dvuh principah psihicheskoy deyatelnosti.txt'...\n",
      "Process file 'Freyd Zigmund. Moisey i monoteizm.txt'...\n",
      "Process file 'Freyd Zigmund. Zigmund Freyd. Moisey i monoteizm.txt'...\n",
      "Process file 'Freyd Zigmund. Iz istorii odnogo detskogo nevroza.txt'...\n",
      "Process file 'Freyd Zigmund. Po tu storonu principa udovolstviya.txt'...\n",
      "Process file 'Freyd Zigmund. Infantilnoe vozvraschenie totema.txt'...\n",
      "Process file 'Freyd Zigmund. Tri ocherka po teorii seksualnosti.txt'...\n",
      "Process file 'Freyd Zigmund. My i smert.txt'...\n",
      "Process file 'Freyd Zigmund. Leonardo da Vinchi. Vospominanie detstva.txt'...\n",
      "Process file 'Freyd Zigmund. Fragment analiza isterii (Istoriya bolezni Dory).txt'...\n",
      "Process file 'Freyd Zigmund. Zabyvanie inostrannyh slov.txt'...\n",
      "Process file 'Freyd Zigmund. Buduschee odnoy illyuzii.txt'...\n",
      "Process file 'Freyd Zigmund. Vvedenie v psihoanaliz (Lekcii 1-15 chast 1 2).txt'...\n",
      "Process file 'Freyd Zigmund. Vytesnenie.txt'...\n",
      "Process file 'Freyd Zigmund. EKONOMIChESKAYa PROBLEMA MAZOHIZMA.txt'...\n",
      "Process file 'Freyd Zigmund. Massovaya psihologiya i analiz chelovecheskogo Ya.txt'...\n",
      "Process file 'Freyd Zigmund. Iz zhizni detskoy dushi.txt'...\n",
      "Process file 'Freyd Zigmund. Psihologiya bessoznatelnogo.txt'...\n",
      "Process file 'Freyd Zigmund. Iz zhizni detskoy dushi (Dva sluchaya detskoy lzhi).txt'...\n",
      "Process file 'Freyd Zigmund. Vvedenie v psihologiyu.txt'...\n",
      "Process file 'Freyd Zigmund. Vvedenie v psihoanaliz (Chast 4 lekcii 29-35).txt'...\n",
      "Process file 'Freyd Zigmund. O snovidenii.txt'...\n",
      "Process file 'Freyd Zigmund. Massovaya psihologiya.txt'...\n",
      "Process file 'Freyd Zigmund. Primenenie tolkovaniya snovideniy pri psihoanalize.txt'...\n",
      "Process file 'Freyd Zigmund. Vvedenie v psihoanaliz (Lekcii 16-28 chast 3).txt'...\n",
      "Process file 'Freyd Zigmund. O prevraschenii vlecheniy v osobennosti analnoy erotiki.txt'...\n",
      "Process file 'Freyd Zigmund. Bred i sny v Gradive V. Iensena.txt'...\n",
      "Process file 'Freyd Zigmund. Tolkovanie snovideniy.txt'...\n",
      "Process file 'Freyd Zigmund. Neizbezhna li voyna.txt'...\n",
      "Process file 'Freyd Zigmund. Psihologiya mass i analiz chelovecheskogo Ya.txt'...\n",
      "Process file 'Freyd Zigmund. Ocherk istorii psihoanaliza.txt'...\n",
      "Process file 'Freyd Zigmund. Dostoevskiy i otceubiystvo.txt'...\n",
      "Finish: 58 files processed, 2487676 tokens collected.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class BooksStorage:\n",
    "    def __init__(self, tokenizer: AutoTokenizer, output_file: str, min_doc_len: int = 100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._output_file = output_file\n",
    "        if os.path.isfile(self._output_file):\n",
    "            os.remove(self._output_file)\n",
    "        self._min_doc_len = min_doc_len\n",
    "        self._tokens_datatype = np.int32\n",
    "        self._tokens_datasize = 4\n",
    "        self._base_length = 0\n",
    "\n",
    "    def from_txt_files(self, folder: str, encoding: str = \"utf8\"):\n",
    "        num = 0\n",
    "        print(f\"Strarting to process folder '{folder}':\")\n",
    "        \n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                print(f\"Process file '{file}'...\")\n",
    "                file_path = os.path.join(folder, file)\n",
    "                text = self._read_txt_file(file_path, encoding)\n",
    "                self._process_text(text)\n",
    "                num += 1\n",
    "        print(f\"Finish: {num} files processed, {self._base_length} tokens collected.\")\n",
    "    \n",
    "    def _read_txt_file(self, file_path: str, encoding: str):\n",
    "        with open(file_path, \"r\", encoding=encoding) as f:\n",
    "            return(f.read())\n",
    "        \n",
    "    def _process_text(self, text: str):\n",
    "        docs = []\n",
    "        for part in text.split(\"\\n\"):\n",
    "            if len(part) >= self._min_doc_len:\n",
    "                docs.append(part)\n",
    "        for doc_ids in tokenizer(docs, padding=False).input_ids:\n",
    "            # print(doc_ids)\n",
    "            self._write_to_output_file(doc_ids + [tokenizer.eos_token_id])\n",
    "\n",
    "    def _write_to_output_file(self, tokens: list):\n",
    "        part = np.array(tokens, dtype=self._tokens_datatype)\n",
    "        with open(self._output_file, \"ab\") as f:\n",
    "            part.tofile(f)\n",
    "        self._base_length += len(part)\n",
    "        \n",
    "\n",
    "    def get_chunk(self, position: int, length: int) -> list[int]:\n",
    "        return np.fromfile(\n",
    "            self._output_file, \n",
    "            dtype = self._tokens_datatype, \n",
    "            offset = self._tokens_datasize * position,\n",
    "            count = length\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        return self._base_length\n",
    "\n",
    "\n",
    "\n",
    "books_storage = BooksStorage(tokenizer, \"temp/freud.data\")\n",
    "books_storage.from_txt_files(\"data/Freud\", \"windows-1251\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class BooksDataset(Dataset):\n",
    "    def __init__(self, \n",
    "    books_storage: BooksStorage,\n",
    "    chunk_size: int,\n",
    "    indexes: list[int]):\n",
    "        self._books_storage = books_storage\n",
    "        self._chunk_size = chunk_size\n",
    "        self._indexes = indexes\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_index = self._indexes[index]\n",
    "        position = target_index * self._chunk_size\n",
    "        input_ids = self._books_storage.get_chunk(position, self._chunk_size)\n",
    "        label_ids = [-100 if t == self._books_storage.tokenizer.eos_token_id else t for t in input_ids]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "class BooksCollector:\n",
    "    def __init__(self, \n",
    "    books_storage: BooksStorage, \n",
    "    chunk_size: int):\n",
    "        self._books_storage = books_storage\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunks_number = self._books_storage.length // self._chunk_size\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self._chunks_number\n",
    "\n",
    "    def train_test_split(self, test_part: float = 0.2, shuffle: bool = True):\n",
    "        indexes = [i for i in range(self._chunks_number)]\n",
    "        if shuffle:\n",
    "            indexes = sample(indexes, k = self._chunks_number)\n",
    "        split_position = int(self.length * test_part)\n",
    "        test_dataset = BooksDataset(self._books_storage, self._chunk_size, indexes[:split_position])\n",
    "        train_dataset = BooksDataset(self._books_storage, self._chunk_size, indexes[split_position:])\n",
    "        return  train_dataset, test_dataset\n",
    "\n",
    "\n",
    "books_collector = BooksCollector(books_storage, 2048)\n",
    "train_dataset, test_dataset = books_collector.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asur/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  2%|▏         | 50/2430 [00:27<21:36,  1.84it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtemp\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     num_train_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     per_device_eval_batch_size  \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     12\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     14\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m     15\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtest_dataset\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2717\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2719\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"temp\",\n",
    "    num_train_epochs = 5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size  = 2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['return_output_length'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39m\u001b[39mПсихоаналитическое исследование с самого начала указывало\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[0;32m----> 2\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      3\u001b[0m     torch\u001b[39m.\u001b[39;49mtensor([tokens])\u001b[39m.\u001b[39;49mcuda(),\n\u001b[1;32m      4\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     return_output_length\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m tokenizer\u001b[39m.\u001b[39mdecode(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/generation/utils.py:1231\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m-> 1231\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1233\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/generation/utils.py:1109\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1110\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1111\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1112\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['return_output_length'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"Психоаналитическое исследование с самого начала указывало\").input_ids\n",
    "output = model.generate(\n",
    "    torch.tensor([tokens]).cuda(),\n",
    "    max_new_tokens=40,\n",
    "    return_output_length=True\n",
    ")[0]\n",
    "\n",
    "tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
